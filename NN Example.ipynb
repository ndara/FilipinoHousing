{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x);\n",
    "\n",
    "def relu_prime(fx):\n",
    "    return (fx != 0) * 1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(fx):\n",
    "    return fx * (1 - fx)\n",
    "    \n",
    "def tanh(x):\n",
    "    return 2 / (1+np.exp(-2*x)) - 1\n",
    "    \n",
    "def tanh_prime(fx):\n",
    "    return 1 - fx ** 2\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def identity_prime(x):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(truth, prediction):\n",
    "    return ((prediction - truth) ** 2).mean()\n",
    "\n",
    "def mse_derivative(truth, prediction):\n",
    "    return (2 * (prediction - truth)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    \"relu\" : np.vectorize(relu),\n",
    "    \"sigmoid\" : np.vectorize(sigmoid),\n",
    "    \"tanh\" : np.vectorize(tanh),\n",
    "    \"identity\" : np.vectorize(identity)\n",
    "}\n",
    "\n",
    "activation_derivatives = {\n",
    "    \"relu\" : np.vectorize(relu_prime),\n",
    "    \"sigmoid\" : np.vectorize(sigmoid_prime),\n",
    "    \"tanh\" : np.vectorize(tanh_prime),\n",
    "    \"identity\" : np.vectorize(identity_prime)\n",
    "}\n",
    "\n",
    "cost_functions = {\n",
    "    \"mse\" : mse\n",
    "}\n",
    "\n",
    "cost_derivatives = {\n",
    "    \"mse\" : mse_derivative\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  \n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative = None, bias = True):\n",
    "        \"\"\"Initializes a layer to be densely connected into a neural network.\n",
    "        \n",
    "        Args:\n",
    "          - size: the number of neurons in the layer (does not include the bias if there is one)\n",
    "          - activation_function: the activation function to be applied to the weighted sum. Must pass either a string or a np.vectorized function.\n",
    "          - activation_derivative: if passed a non-string to activation_function, this must be a np.vectorized function as well\n",
    "          - bias: determines is a bias node will be present in this layer or not        \n",
    "        \"\"\"\n",
    "        if isinstance(activation_function, str):\n",
    "            self.activation_function = activation_functions[activation_function]\n",
    "            self.activation_derivative = activation_derivatives[activation_function]\n",
    "        elif activation_derivative is None:\n",
    "            raise ParameterError\n",
    "        else:\n",
    "            self.activation_function = activation_function\n",
    "            self.activation_derivative = activation_derivative\n",
    "            \n",
    "        self.bias = bias\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size+1 if bias else output_size\n",
    "        self.weights = np.random.randn(self.input_size, output_size) # NOT self.output_size\n",
    "        self.inputs = None\n",
    "    \n",
    "    def forward_prop(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return self.activate(inputs.dot(self.weights))\n",
    "        \n",
    "    def activate(self, weighted_sums):\n",
    "        \"\"\"Passes a matrix of weighted sums through an activation function. Adds a bias column to the end if wanted.\n",
    "        \n",
    "        Args:\n",
    "          - weighted_sums: a matrix composed of all nodes of prior layer multiplied by their synapses and summed up\n",
    "          \n",
    "        Returns:\n",
    "          A matrix of the same size (plus one column if bias is wanted) where each value has been activated.\n",
    "        \"\"\"\n",
    "        activated_values = self.activation_function(weighted_sums)\n",
    "        if self.bias:\n",
    "            bias_column = np.ones((weighted_sums.shape[0],1))\n",
    "            activated_values = np.append(activated_values, bias_column, axis = 1)\n",
    "        \n",
    "        self.past = activated_values\n",
    "        \n",
    "        return activated_values\n",
    "    \n",
    "    def update_weights(self, dldh_prod, eta):\n",
    "        \"\"\"Update the weights based on the partial derivatives of the outputs of the nodes within this layer.\n",
    "        \n",
    "        Args:\n",
    "          - dldh: a vector containing the partial derivatives of each node (excluding the bias node)\n",
    "        \"\"\"\n",
    "        \n",
    "        dldw = self.inputs.T.dot(dldh_prod)\n",
    "        self.weights -= eta * dldw\n",
    "        \n",
    "    \n",
    "    def back_prop(self, dldh, eta):\n",
    "        \"\"\"Calculate the partial derivatives of the prior layer and signal to update this layer's weights.\n",
    "        \n",
    "        Args:\n",
    "          - dldh: a vector containing the partial derivatives of each node (including the bias node)\n",
    "        \n",
    "        Return:\n",
    "          A vector for the prior layer containing the partial derivatives of each of their nodes.\n",
    "        \"\"\"\n",
    "        past = self.past\n",
    "        if self.bias: #remove bias\n",
    "            dldh = dldh[:, :-1]\n",
    "            past = past[:, :-1]\n",
    "            \n",
    "        activation_derivatives = self.activation_derivative(past)\n",
    "        dldh_prod = np.multiply(activation_derivatives, dldh)\n",
    "        self.update_weights(dldh_prod, eta)\n",
    "        \n",
    "        prior_dldh = self.weights.dot(dldh_prod.T).T\n",
    "        \n",
    "        return prior_dldh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_units, cost = \"mse\", input_bias = True):\n",
    "        \"\"\"Initializes a neural network model with no hidden layers. Must add them manually to define one completely.\n",
    "        \n",
    "        Args: \n",
    "          - input_units: number of neurons in the input layer\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.dims = [input_units+1 if input_bias else input_units]\n",
    "        self.cost = cost_functions[cost]\n",
    "        self.cost_derivative = cost_derivatives[cost]\n",
    "        self.input_bias = input_bias\n",
    "        \n",
    "    def add_layer(self, nodes, activation_function = \"relu\", bias = True):\n",
    "        \"\"\"Adds a layer to the network. Assumes it is to be fully connected.\n",
    "        \n",
    "        Args:\n",
    "          - nodes: the size of the layer\n",
    "          - activation_function: the activation function specified via string\n",
    "          - bias: whether to have a bias or not\n",
    "        \n",
    "        Returns:\n",
    "          A reference to this object to chain method calls.\n",
    "        \"\"\"\n",
    "        layer = Layer(self.dims[-1], nodes, activation_function, bias = bias)\n",
    "        self.layers.append(layer)\n",
    "        self.dims.append(layer.output_size)\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts output for the given input x.\n",
    "        \n",
    "        Args:\n",
    "          - x: an array of length `input_units`\n",
    "          \n",
    "        Returns:\n",
    "          Predicted output for the given input.\n",
    "        \"\"\"\n",
    "        \n",
    "        # assume x is a horizontal vector\n",
    "        output = x\n",
    "        \n",
    "        if self.input_bias:\n",
    "            output = np.append(output, np.ones((x.shape[0], 1)), axis = 1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output = layer.forward_prop(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def fit(self, X, Y, eta, epochs, batch_size = 1):\n",
    "        \"\"\"Trains the network based on the input data against the truth given.\n",
    "        \n",
    "        Args:\n",
    "          - X: a matrix of shape [data points, features]\n",
    "          - Y: an array of length [data points]\n",
    "          - epochs: number of times to iterate over the entire dataset\n",
    "          - batch_size: the number of data points to step through before updating the weights\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                end_point = min(i + batch_size, X.shape[0])\n",
    "                self.update(X[i:end_point,:], Y[i:end_point], eta)\n",
    "        \n",
    "    def update(self, x, y, eta):\n",
    "        \"\"\"Updates neural network weights based on new training data.\n",
    "        \n",
    "        Args:\n",
    "          - x: an array of length `input_units`\n",
    "          - y: a float representing the output\n",
    "        \"\"\"\n",
    "        prediction = self.predict(x)\n",
    "        dldh = self.cost_derivative(y, prediction)\n",
    "        for i, layer in enumerate(self.layers[::-1]):\n",
    "            dldh = layer.back_prop(dldh, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetwork at 0x7f454e77a828>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.add_layer(10, \"tanh\").add_layer(5, \"tanh\").add_layer(3, \"tanh\", bias = False).add_layer(1, \"tanh\", bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 0, 0],\n",
    "              [1, 0, 0, 0],\n",
    "              [0, 1, 0, 0],\n",
    "              [0, 0, 1, 0],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 1, 0, 0], \n",
    "              [1, 0, 1, 0],\n",
    "              [1, 0, 0, 1],\n",
    "              [0, 1, 1, 0],\n",
    "              [0, 1, 0, 1],\n",
    "              [0, 0, 1, 1],\n",
    "              [1, 1, 1, 0],\n",
    "              [1, 1, 0, 1],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 1, 1, 1],\n",
    "              [1, 1, 1, 1]])\n",
    "\n",
    "Y = np.array([[0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn.fit(X, Y, eta = 0.01, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.   ],\n",
       "       [-0.   ],\n",
       "       [ 0.001],\n",
       "       [ 0.981],\n",
       "       [ 0.983],\n",
       "       [ 0.001],\n",
       "       [ 0.99 ],\n",
       "       [ 0.987],\n",
       "       [ 0.986],\n",
       "       [ 0.991],\n",
       "       [ 0.991],\n",
       "       [ 0.991],\n",
       "       [ 0.991],\n",
       "       [ 0.991],\n",
       "       [ 0.991],\n",
       "       [ 0.991]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(X).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[15:16,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
